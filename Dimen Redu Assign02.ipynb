{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4042db48-66a3-44df-8ab9-49b78498e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. What is a projection and how is it used in PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ce3e066-a14c-4ea5-a4a7-9955f7258769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This paper introduces a Projected Principal Component Analysis (Projected-PCA), which employees principal \n",
    "#component analysis to the projected (smoothed) data matrix onto a given linear space spanned by covariates. When \n",
    "#it applies to high-dimensional factor analysis, the projection removes noise components.\n",
    "#The steps to perform PCA are as follows.\n",
    "#Compute the covariance matrix. ...\n",
    "#Find eigenvectors ( ) and eigenvalues ( ) of the covariance matrix using singular value decomposition. ...\n",
    "#Select first columns from eigenvector matrix. ...\n",
    "#Compute projections of original observation onto new vector form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e6cce1e-e0d2-431e-ac71-4ddc08b61d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. How does the optimization problem in PCA work, and what is it trying to achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "781e21e4-2cf8-426d-9320-078e57cbc1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA can be used to reduce the dimensionality of the data by creating a set of derived variables that are linear \n",
    "#combinations of the original variables.\n",
    "#identifying the main axes of variance within a data set and allows for easy data exploration to understand the\n",
    "#key variables in the data and spot outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80cd2527-2327-4ebb-ab6d-29d75e8d321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. What is the relationship between covariance matrices and PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47f3257c-23e7-49a1-9e2d-04197d35864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA is simply described as “diagonalizing the covariance matrix”. What does diagonalizing a matrix mean in this\n",
    "#context? It simply means that we need to find a non-trivial linear combination of our original variables such \n",
    "#that the covariance matrix is diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d12323b-ba88-423c-bc76-2801fb1e06b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. How does the choice of number of principal components impact the performance of PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ff25e1b-0a25-4979-beac-8f3572e91622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A widely applied approach is to decide on the number of principal components by examining a scree plot. \n",
    "#By eyeballing the scree plot, and looking for a point at which the proportion of variance explained by each \n",
    "#subsequent principal component drops off. This is often referred to as an elbow in the scree plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28d5e304-205e-4d27-a634-33443dc856f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. How can PCA be used in feature selection, and what are the benefits of using it for this purpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa14ca6f-1770-4829-a9ea-831e22538301",
   "metadata": {},
   "outputs": [],
   "source": [
    "#he only way PCA is a valid method of feature selection is if the most important variables are the ones that\n",
    "#happen to have the most variation in them .\n",
    "#Principal components are independent of each other, so removes correlated features. PCA improves the performance\n",
    "#of the ML algorithm as it eliminates correlated variables that don't contribute in any decision making. PCA helps\n",
    "#in overcoming data overfitting issues by decreasing the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "516fd350-2d74-4c48-bd28-37f36361168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. What are some common applications of PCA in data science and machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84382f6c-bf78-48b4-802e-0f1d8094709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applications of PCA in Machine Learning\n",
    "#PCA is used to visualize multidimensional data.\n",
    "#It is used to reduce the number of dimensions in healthcare data.\n",
    "#PCA can help resize an image.\n",
    "#It can be used in finance to analyze stock data and forecast returns.\n",
    "#PCA helps to find patterns in the high-dimensional datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54c0e7c6-3aec-4943-bc43-b2098a8e4027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7.What is the relationship between spread and variance in PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "076c4fad-a81f-4737-8923-0c13eed1f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variance for variable X. Unlike variance, which measures spread in one dimension, covariance measures the \n",
    "#deviation from the mean of two variables with respect to each other. In other words, covariance measures the\n",
    "#relationship between the two dimensions.\n",
    "# the larger the variance explained by a principal component, the more important that component is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12c04f8a-ccc6-4dca-b8cf-4f6a53fa19ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8. How does PCA use the spread and variance of the data to identify principal components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a8aa3b1-4978-46c7-ad83-fc094666dc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA works by finding the directions of maximum variance in the data set and projecting the data onto these \n",
    "#directions. The principal components are ordered by the amount of variance they explain and are used for feature \n",
    "#selection, data compression, clustering, and classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cecadbde-5ba0-4c0e-adb1-2b520ddfe4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9. How does PCA handle data with high variance in some dimensions but low variance in others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869739cd-1b14-4661-8f02-89ed4641e841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
